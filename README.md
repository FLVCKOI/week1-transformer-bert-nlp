# ğŸ¬ BERT for IMDb Sentiment Classification

æœ¬é¡¹ç›®ä½¿ç”¨é¢„è®­ç»ƒçš„ BERT æ¨¡å‹å¯¹ IMDb å½±è¯„æ•°æ®é›†è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»ï¼ˆæ­£é¢ / è´Ÿé¢ï¼‰ï¼Œæ¶µç›–å®Œæ•´çš„å¾®è°ƒã€è¯„ä¼°å’Œéƒ¨ç½²æµç¨‹ï¼Œå¹¶è¾…ä»¥ç†è®ºç¬”è®°ä¸å®éªŒåˆ†æï¼Œé€‚åˆ NLP åˆå­¦è€…å’Œ Transformers å®è·µè€…å‚è€ƒã€‚

---

## ğŸ“ é¡¹ç›®ç»“æ„



.
â”œâ”€â”€ train.py               # è®­ç»ƒä¸è¯„ä¼°è„šæœ¬ï¼ˆTensorFlowï¼‰
â”œâ”€â”€ infer.py               # æ¨ç†è„šæœ¬ï¼ˆè¾“å…¥å½±è¯„è¿”å›é¢„æµ‹æ ‡ç­¾ï¼‰
â”œâ”€â”€ training\_report.md     # åˆ†ç±»æŠ¥å‘Š
â”œâ”€â”€ theory\_notes.md        # ç†è®ºå­¦ä¹ æ•´ç†ï¼ˆTransformer / BERT / Trainer ç­‰ï¼‰
â”œâ”€â”€ requirements.txt       # æ‰€éœ€ Python åº“
â”œâ”€â”€ output/                # è®­ç»ƒåæ¨¡å‹åŠåˆ†è¯å™¨ä¿å­˜ä½ç½®
â””â”€â”€ plots/                 # è®­ç»ƒè¿‡ç¨‹ loss / accuracy å¯è§†åŒ–å›¾

````

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–ç¯å¢ƒï¼š

```bash
pip install -r requirements.txt
````

### è®­ç»ƒæ¨¡å‹ï¼š

```bash
python train.py
```

### æ¨ç†ç¤ºä¾‹ï¼š

```bash
python infer.py
```

---

## ğŸ” ç¤ºä¾‹è¾“å‡º

```text
è¾“å…¥å½±è¯„ï¼š This movie is absolutely wonderful and inspiring.
æ¨¡å‹é¢„æµ‹ï¼š positive
```

---

## ğŸ“ æŠ€æœ¯æ ˆ

* **Transformers**ï¼šBERT-base-uncasedï¼ˆHuggingface ğŸ¤— Transformersï¼‰
* **æ•°æ®é›†**ï¼šIMDb Large Movie Review Dataset
* **æ¡†æ¶**ï¼šTensorFlow 2.x + Huggingface ğŸ¤—
* **å¯è§†åŒ–**ï¼šMatplotlib, Seaborn
* **æ¨¡å‹ä¿å­˜æ ¼å¼**ï¼š`tf_model.h5` + `tokenizer.json`

---

## ğŸ“Š å®éªŒç»“æœ

* **Accuracy**: 90.3%
* **Precision / Recall / F1**ï¼šè¯¦è§ `training_report.md`

### ğŸ“ˆ è®­ç»ƒè¿‡ç¨‹å›¾è¡¨ï¼š

* `plots/__results___6_0.png`ï¼ˆLoss æ›²çº¿ï¼‰
* `plots/__results___6_1.png`ï¼ˆAccuracy æ›²çº¿ï¼‰
* `plots/__results___8_1.png`ï¼ˆConfusion Matrixï¼‰

---

## ğŸ“˜ ç†è®ºæ•´ç†

* Transformer Attention æ¨å¯¼ï¼ˆQ/K/Vï¼Œå¤šå¤´ï¼‰
* BERT / GPT ç»“æ„ä¸è®­ç»ƒå·®å¼‚
* MLM ä¸ NSP åŸç†
* Huggingface `Trainer` æ¨¡å—æ„æˆ
* é•¿æ–‡æœ¬å¤„ç†æ–¹æ¡ˆï¼ˆå¦‚ Longformer / BigBirdï¼‰
* æ¨¡å‹éƒ¨ç½²å»ºè®®ï¼ˆFastAPI, æ¨¡å‹å‹ç¼©ä¼˜åŒ–ï¼‰
* å¾®è°ƒæŠ€å·§ï¼ˆç±»åˆ«ä¸å¹³è¡¡å¤„ç†ã€è¯„ä»·æŒ‡æ ‡é€‰æ‹©ï¼‰

---

## ğŸ§ª æ¨ç†ä»£ç ç¤ºä¾‹

```python
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
import tensorflow as tf

tokenizer = AutoTokenizer.from_pretrained("output/")
model = TFAutoModelForSequenceClassification.from_pretrained("output/")

text = "I really enjoyed this movie."
inputs = tokenizer(text, return_tensors="tf", truncation=True, padding=True)
logits = model(**inputs).logits
pred = tf.argmax(logits, axis=1).numpy()[0]
print("Predicted:", "positive" if pred == 1 else "negative")
```

---

## ğŸ“¦ æ¨¡å‹å¯¼å‡ºæ–‡ä»¶è¯´æ˜

`output/` æ–‡ä»¶å¤¹åŒ…å«å®Œæ•´æ¨¡å‹æ–‡ä»¶ï¼š

* `tf_model.h5`
* `tokenizer_config.json`
* `vocab.txt`
* `config.json`

---

## ğŸ“š æ¨èèµ„æ–™

* [Huggingface Transformers å®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/transformers/index)
* [Stanford CS224n NLP è¯¾ç¨‹](http://web.stanford.edu/class/cs224n/)
* [Transformers å®æˆ˜è¯¾ç¨‹ï¼ˆæ¨è Bç«™ï¼‰](https://www.bilibili.com)

---

## ğŸ™‹ ä½œè€…ç®€ä»‹

æœ¬é¡¹ç›®ç”±å¦é—¨å¤§å­¦æœ¬ç§‘ç”Ÿä¸»å¯¼å¼€å‘ï¼Œä½œä¸ºä»é›¶å¼€å§‹å­¦ä¹ å¤§æ¨¡å‹ä¸ Transformers çš„å®æˆ˜æ€»ç»“ã€‚

æ¬¢è¿è”ç³»ä¸äº¤æµå­¦ä¹ ç»éªŒã€æ¨¡å‹éƒ¨ç½²ç­‰ï¼

```

---

å¦‚éœ€æˆ‘å¸®ä½ **ç”Ÿæˆä¸€ä¸ªå®Œæ•´ä»“åº“ç»“æ„æ‰“åŒ…**æˆ–è‡ªåŠ¨ä¸Šä¼ åˆ°ä½ çš„ GitHub è´¦å·ï¼Œä¹Ÿå¯ä»¥ç»§ç»­å‘Šè¯‰æˆ‘ã€‚éœ€è¦æˆ‘ä¹Ÿå¸®ä½ ç”Ÿæˆ `training_report.md` å’Œ `theory_notes.md` çš„æ¨¡æ¿å—ï¼Ÿ
```
