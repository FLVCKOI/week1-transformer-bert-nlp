ğŸ¬ BERT for IMDb Sentiment Classification
æœ¬é¡¹ç›®ä½¿ç”¨é¢„è®­ç»ƒçš„ BERT æ¨¡å‹å¯¹ IMDb å½±è¯„æ•°æ®é›†è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»ï¼ˆæ­£é¢ / è´Ÿé¢ï¼‰ï¼Œæ¶µç›–å®Œæ•´çš„å¾®è°ƒã€è¯„ä¼°å’Œéƒ¨ç½²æµç¨‹ï¼Œå¹¶è¾…ä»¥ç†è®ºç¬”è®°ä¸å®éªŒåˆ†æï¼Œé€‚åˆ NLP åˆå­¦è€…å’Œ Transformers å®è·µè€…å‚è€ƒã€‚

ğŸ“ é¡¹ç›®ç»“æ„
. â”œâ”€â”€ train.py # è®­ç»ƒä¸è¯„ä¼°è„šæœ¬ï¼ˆTensorFlowï¼‰ â”œâ”€â”€ infer.py # æ¨ç†è„šæœ¬ï¼ˆè¾“å…¥å½±è¯„è¿”å›é¢„æµ‹æ ‡ç­¾ï¼‰ â”œâ”€â”€ training_report.md # åˆ†ç±»æŠ¥å‘Š â”œâ”€â”€ theory_notes.md # ç†è®ºå­¦ä¹ æ•´ç†ï¼ˆTransformer / BERT / Trainer ç­‰ï¼‰ â”œâ”€â”€ requirements.txt # æ‰€éœ€ Python åº“ â”œâ”€â”€ output/ # è®­ç»ƒåæ¨¡å‹åŠåˆ†è¯å™¨ä¿å­˜ä½ç½® â””â”€â”€ plots/ # è®­ç»ƒè¿‡ç¨‹ loss / accuracy å¯è§†åŒ–å›¾ 
ğŸš€ å¿«é€Ÿå¼€å§‹
å®‰è£…ä¾èµ–ç¯å¢ƒï¼š
pip install -r requirements.txt
è®­ç»ƒæ¨¡å‹ï¼š
python train.py
æ¨ç†ç¤ºä¾‹ï¼š
python infer.py
ğŸ” ç¤ºä¾‹è¾“å‡º
è¾“å…¥å½±è¯„ï¼š This movie is absolutely wonderful and inspiring.
æ¨¡å‹é¢„æµ‹ï¼š positive

ğŸ“ æŠ€æœ¯æ ˆ
Transformersï¼šBERT-base-uncasedï¼ˆHuggingface Transformersï¼‰
æ•°æ®é›†ï¼šIMDb Large Movie Review Dataset
æ¡†æ¶ï¼šTensorFlow 2.x + Huggingface ğŸ¤—
å¯è§†åŒ–ï¼šMatplotlib, Seaborn
æ¨¡å‹ä¿å­˜æ ¼å¼ï¼štf_model.h5 + tokenizer.json
ğŸ“Š å®éªŒç»“æœ
Accuracy: 90.3%
Precision / Recall / F1ï¼šè¯¦è§ training_report.md
è®­ç»ƒè¿‡ç¨‹å›¾è¡¨ï¼š

plots/__results___6_0.pngï¼ˆLoss æ›²çº¿ï¼‰
plots/__results___6_1.pngï¼ˆAccuracy æ›²çº¿ï¼‰
plots/__results___8_1.pngï¼ˆConfusion Matrixï¼‰
ğŸ“˜ ç†è®ºæ•´ç†
Transformer Attention æ¨å¯¼ï¼ˆQ/K/Vï¼Œå¤šå¤´ï¼‰
BERT / GPT ç»“æ„ä¸è®­ç»ƒå·®å¼‚
MLM ä¸ NSP åŸç†
Huggingface Trainer çš„æ¨¡å—æ„æˆ
é•¿æ–‡æœ¬å¤„ç†æ–¹æ¡ˆï¼ˆå¦‚ Longformer / BigBirdï¼‰
éƒ¨ç½²å»ºè®®ï¼ˆFastAPI, æ¨¡å‹å‹ç¼©ä¼˜åŒ–ï¼‰
å¾®è°ƒæŠ€å·§ï¼ˆç±»åˆ«ä¸å¹³è¡¡å¤„ç†ã€è¯„ä»·æŒ‡æ ‡é€‰æ‹©ï¼‰
ğŸ§ª æ¨ç†ä»£ç ç¤ºä¾‹
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification tokenizer = AutoTokenizer.from_pretrained("output/") model = TFAutoModelForSequenceClassification.from_pretrained("output/")
text = "I really enjoyed this movie."
inputs = tokenizer(text, return_tensors="tf", truncation=True, padding=True)
logits = model(**inputs).logits
pred = tf.argmax(logits, axis=1).numpy()[0]
print("Predicted:", "positive" if pred == 1 else "negative")
ğŸ“¦ æ¨¡å‹å¯¼å‡ºæ–‡ä»¶
output/ æ–‡ä»¶å¤¹åŒ…å«å®Œæ•´æ¨¡å‹ï¼š

tf_model.h5
tokenizer_config.json
vocab.txt
config.json
ğŸ“š æ¨èèµ„æ–™
Huggingface Transformers å®˜æ–¹æ–‡æ¡£
Stanford CS224n NLP è¯¾ç¨‹
Transformers å®æˆ˜è¯¾ç¨‹
ğŸ™‹ ä½œè€…
æœ¬é¡¹ç›®ç”±å¦é—¨å¤§å­¦æœ¬ç§‘ç”Ÿä¸»å¯¼å¼€å‘ï¼Œä½œä¸ºä»é›¶å¼€å§‹å­¦ä¹ å¤§æ¨¡å‹ä¸ Transformers çš„å®æˆ˜æ€»ç»“ã€‚

æ¬¢è¿è”ç³»ä¸äº¤æµå­¦ä¹ ç»éªŒã€æ¨¡å‹éƒ¨ç½²ç­‰ï¼